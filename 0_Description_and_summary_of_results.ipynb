{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict Friday's oil price\n",
    "##  DataChallenge\n",
    "### Natacha Altamirano\n",
    "\n",
    "<img src=\"images/op.jpg\">\n",
    "\n",
    "## DESCRIPTION \n",
    "(source [Invesopidia](https://www.investopedia.com/articles/economics/08/determining-oil-prices.asp#ixzz5Wu4aSckV ))\n",
    "\n",
    "\n",
    "\n",
    "The Goal of this challenge is to predict the price of crude oil on Friday Nov 16th 2018. Crude oil is a high-demand comodity and its price as a huge impact in the economy worldwide. The price in in generally quoted as price per barrel and tends to be very volatile depending on two major factors:\n",
    "\n",
    "- Supply and Demand\n",
    "- Market feeling\n",
    "\n",
    "As demand increases (or supply decreases) the price should go up. As demand decreases (or supply increases) the price should go down. However, the price of crude oil is set in the futures market, where a future contract gives one the right to purchase oil by the barrel at a predefined price on a predefined date in the future. Under a futures contract, both the buyer and the seller are obligated to fulfill their side of the transaction on the specified date. Some of the entities trading these contracts are speculators and probably whose are the once in charge of the high volatility od oil price.\n",
    "\n",
    "On the market side, the  biggest influencer of oil prices is OPEC, made up of 13 countries (Algeria, Angola, Ecuador, Indonesia, Iran, Iraq, Kuwait, Libya, Nigeria, Qatar, Saudi Arabia, the United Arab Emirates and Venezuela); collectively, OPEC controls 40% of the world's supply of oil.\n",
    "\n",
    "\n",
    "## DATA\n",
    "\n",
    "A forecasting of oil price its not an easy task for three reasons\n",
    "\n",
    "- High volatility\n",
    "- No seasonality\n",
    "- Controlling comodity (The price of the oil has a bigger impact in other comodity prices and not viceversa)\n",
    "\n",
    "We can try to model both the market feeling and the supply demand, for example\n",
    "\n",
    "- **S/D** get time series data of energy consuption for US (as a proxy of world energy consuption), get oild reserves data for US, use future contracts past data to estimate the price of future contracts for the future. \n",
    "\n",
    "- **Market** get data of oil reserves for OPEC countries\n",
    "\n",
    "However, given the time-contraint we are going to focus as a first step to build a predicting mode given just data of past crude oil price. For this we retrive data from [OPEC](https://www.investopedia.com/terms/o/opec.asp) basket crude oil price on [QUNADL](https://www.quandl.com/data/OPEC/ORB-OPEC-Crude-Oil-Price). This is a weighted avarage of the crude oil barrel price for the different countries in OPEC. We could have consider other data sets like the [WTI](https://www.investopedia.com/terms/w/wti.asp) or [Brent](https://www.investopedia.com/terms/b/brentblend.asp) but we are considering OPEC since they own 40%. However an interesting direction is combining this threee data sets. \n",
    "\n",
    "\n",
    "### Data exploration\n",
    "\n",
    "[NOTEBOOK]()\n",
    "\n",
    "The OPEC data looks like (we have data up to Nov. 8 2018 6 days before the wanted values)\n",
    "\n",
    "<img src=\"images/series.pdf\">\n",
    "\n",
    "Exploring the seasonality we realize there is no clear pattern that could indicate a monthly seasonality in the oil price. \n",
    "\n",
    "<img src=\"images/seasonality.pdf\">\n",
    "\n",
    "This could tell us that there is not clear correlation between energy demand (high in winter) and the price of OPEC crude oil, or, if it is, it is hidden behind some bigger effects.\n",
    "\n",
    "#### Correlation an autocorrelation\n",
    "\n",
    "Lets now measure the autocorrelation of our time series. This information will tell us how correlated is an event at time t with the event at time t-1. In the following we show the correlation plot at time t and several lag steps\n",
    "\n",
    "<img src=\"images/auto.pdf\">\n",
    "\n",
    "We can explore this autocorrelation function as function of the lag\n",
    "\n",
    "<img src=\"images/autocor.pdf\">\n",
    "\n",
    "\n",
    "We see that the autocorrelation is negative after about 2 years. However, there is no clear patter of seasonality either here.\n",
    "\n",
    "\n",
    "## MODEL\n",
    "\n",
    "Eventhough there are *classical* models available in this challenge we are going to make use of a LSTM (Long-Short Term Memory). An LSTM is a Recursive Neural Network (RNN) modify in such a way that is allowed to forget what it learnt after some period of time. This makes an LSTM faster to run and also numerically stable. \n",
    "An RNN is esentially a neural network that is allowed to learn from the past:\n",
    "\n",
    "<img src=\"images/images.png\">\n",
    "\n",
    "This network will allow us to learn information about different frequencies of our time series as opposed to classical model where ones learns more easily the large scale behaviour. Our implementaion its done in Keras.\n",
    "\n",
    "### Vanilla model\n",
    "\n",
    "[NOTEBOOK]()\n",
    "\n",
    "In this first step we are using an LSTM with 1 feature (lag 1). We are traying with different amount of hidden neurons fixing the batch size and the regularization\n",
    "\n",
    "<img src=\"images/numnur.pdf\">\n",
    "\n",
    "We find that our model is highly overfitting, even with the smolles amount of neurons. It overfitts two things\n",
    "\n",
    "- Large scale (short frequencies) this is the long modulation of the data\n",
    "- Short scale (high frequency) fine tunning detail\n",
    "\n",
    "In what follows we will start tunning hyperparameters to try to avoid overfitting. We divide it in two step\n",
    "\n",
    "- Large scale (short frequencies): change the regularization procedure and batch size\n",
    "- Short scale (high frequency): change the number of fetures (add lags) to the model\n",
    "\n",
    "[NOTEBOOK]()\n",
    "\n",
    "We could try doing a grid search of this hyperparameters, however a very simple grid search will involve comparing thousands of LSTM models and its simple computationally very expensive. That is why we have decided changing hyperparameters and comparing to our vanilla model. **This is something that should be included in the next step for the model**\n",
    "\n",
    "### Changing Batch size\n",
    "\n",
    "<img src=\"images/bs.pdf\">\n",
    "          \n",
    "We see that small and large enough batch sizes help wth overfitting. However the model with batch size of 50 takes more epochs to converge and thus is less efficient than a model with less batches. \n",
    "\n",
    "<img src=\"images/lossbatch.pdf\">\n",
    "\n",
    "For this reason we pick a batch size of 10. \n",
    "\n",
    "### Regularizing the model\n",
    "\n",
    "There are different regularization methods for neural networks. In this case we try the bias regularization and kernel regularization and compare how this affect our vanilla model\n",
    "\n",
    "<img src=\"images/kbr.pdf\">\n",
    "\n",
    "We find that\n",
    "\n",
    "\n",
    "- Bias regularization in general shifts the model up\n",
    "- Kernel regularization changes the extreme of the tails up/down\n",
    "- The overall effect is to extremelly shift up the prediction\n",
    "\n",
    "\n",
    "**Using this 1-feature model with the best hyperparameter we predict that Friday's oil price is 78 dollars. **\n",
    "\n",
    "### Changing the number of features\n",
    "\n",
    "[NOTEBOOK1]()\n",
    "\n",
    "[NOTEBOOK2]()\n",
    "\n",
    "Previously we have adressed the large scale overfitting with different hyperparamenters and regularization methods. But we have also mention that we have a short scale overfitting. To adress this we are going to add features to our model.\n",
    "\n",
    "We do this by using a series of lagged events. We show below the performance of our model for different number of features \n",
    "\n",
    "<img src=\"images/nf.pdf\">\n",
    "\n",
    "We notice that the more features we have the more short scale structure we lose. This is indeed good since we don't want our model to overfitt in this scales either. WE have chosen to work with a number of 20 features. However, now that some of the short scale structure is washed out we need to find again the number of neurons that make our model *optimal*. Doing a similar anallysis we decide to work with 10 neurons. \n",
    "\n",
    "The final model we use has:\n",
    "\n",
    "- num_neurons=10\n",
    "- batch_size=10\n",
    "- num_features=20\n",
    "- epochs=10\n",
    "- bias and kernel regularization\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"images/fm.pdf\">\n",
    "\n",
    "\n",
    "**Using this model the predicted oil price is 75.27 USD**\n",
    "\n",
    "\n",
    "## Improvements and future directions\n",
    "\n",
    "- Explore in grid the hyperparameters\n",
    "- Aggregate other data sets to our data sets: other comodity prices, oil reserves, future contracts prices, energy consuption, maybe some news NLP (?)\n",
    "- how can I bagged classical and deep models?\n",
    "- There ought to be a frequency analysis of this time series data that might be fun to try\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
